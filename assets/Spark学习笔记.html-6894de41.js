import{_ as a}from"./plugin-vue_export-helper-c27b6911.js";import{o as n,c as s,a as e}from"./app-5dbb6149.js";const p="/assets/image-20240306104245319-a932e9b8.png",i="/assets/image-20240306111636926-204245d5.png",t="/assets/image-20240306164313345-cf71092d.png",l="/assets/image-20240306170546483-6f8db1cc.png",o="/assets/image-20240306170605695-cbfcfd93.png",r="/assets/image-20240306171631987-20480705.png",c="/assets/image-20240306171657469-b0622b7e.png",d={},u=e(`<h2 id="spark概述" tabindex="-1"><a class="header-anchor" href="#spark概述" aria-hidden="true">#</a> Spark概述</h2><p>Spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。</p><h2 id="spark-vs-hadoop" tabindex="-1"><a class="header-anchor" href="#spark-vs-hadoop" aria-hidden="true">#</a> Spark VS Hadoop</h2><p>从时间上来看:</p><ul><li><p>Hadoop</p><ul><li><p>2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发</p></li><li><p>2008年1月，Hadoop成为Apache顶级项目</p></li><li><p>2011年，1.0正式发布</p></li><li><p>2012年3月，稳定版发布</p></li><li><p>2013年10月，发布2.X(Yarn)版本</p></li></ul></li><li><p>Spark</p><ul><li><p>2009年，Spark诞生于伯克利大学的AMPLab实验室</p></li><li><p>2010年，伯克利大学正式开源了Spark项目</p></li><li><p>2013年6月，Spark成为了Apache基金会下的项目</p></li><li><p>2014年2月，Spark以飞快的速度成为了Apache的顶级项目</p></li><li><p>2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark</p></li></ul></li></ul><p>从功能上来看:</p><ul><li>Hadoop <ul><li>Hadoop是由java语言编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架</li><li>作为Hadoop分布式文件系统，<mark>HDFS</mark>处于Hadoop生态圈的最下层，存储着所有的数据，支持着Hadoop的所有服务。它的理论基础源于Google的The Google File System这篇论文，它是GFS的开源实现</li><li><mark>MapReduce</mark>是一种编程模型，Hadoop根据Google的MapReduce论文将其实现， 作为Hadoop的分布式计算模型，是Hadoop的核心。基于这个框架，分布式并行程序的编写变得异常简单。综合了HDFS的分布式存储和MapReduce的分布式计算，Hadoop在处理海量数据时，性能横向扩展变得非常容易。</li><li><mark>HBase</mark>是对Google的Bigtable的开源实现，但又和Bigtable存在许多不同之处。HBase是一个基于HDFS的分布式数据库，擅长实时地随机读/写超大规模数据集。它也是Hadoop非常重要的组件。</li></ul></li><li>Spark <ul><li>Spark是一种由Scala语言开发的快速、通用、可扩展的大数据分析引擎</li><li><mark>Spark Core</mark>中提供了Spark最基础与最核心的功能</li><li><mark>Spark SQL</mark>是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用 SQL或者Apache Hive版本的SQL 方言（HQL）来查询数据。</li><li><mark>Spark Streaming</mark>是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。</li></ul></li></ul><p>从数据处理框架看：</p><ol><li>Hadoop的MapReduce框架和Spark框架都是数据处理框架</li><li>Hadoop MapReduce由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多 并行运行的数据可复用场景（如：机器学习、图挖掘算法、交互式数据挖掘算法）中存在诸多计算效率等问题。所以Spark应运而生，Spark就是在传统的MapReduce计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的RDD计算模型。</li><li>机器学习中 ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。而Spark所基于的Scala 语言恰恰擅长函数的处理。</li><li>Spark是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集（Resilient Distributed Datasets），提供了比MapReduce丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。</li><li>Spark和Hadoop的根本差异是多个作业之间的数据通信问题: Spark 多个作业之间数据 通信是基于内存，而 Hadoop 是基于磁盘。</li><li>Spark Task 的启动时间快。Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程的方式。</li><li>Spark只有在shuffle的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交 互都要依赖于磁盘交互</li><li>Spark的缓存机制比HDFS的缓存机制高效。</li></ol><p>经过上面的比较，我们可以看出在绝大多数的数据计算场景中，Spark 确实会比 MapReduce 更有优势。但是 Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致 Job 执行失败，此时，MapReduce其实是一个更好的选择，所以 Spark 并不能完全替代 MR。</p><h2 id="spark入门sample" tabindex="-1"><a class="header-anchor" href="#spark入门sample" aria-hidden="true">#</a> Spark入门Sample</h2><div class="language-scala line-numbers-mode" data-ext="scala"><pre class="language-scala"><code><span class="token keyword">def</span> main<span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token keyword">val</span> sparkConf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">&quot;local[*]&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">&quot;WordCount&quot;</span><span class="token punctuation">)</span>
    <span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>

    sparkContext
    <span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">&quot;./src/main/resources/word.txt&quot;</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>_<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot; &quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span>groupBy<span class="token punctuation">(</span>word <span class="token keyword">=&gt;</span> word<span class="token punctuation">)</span>
    <span class="token punctuation">.</span>map<span class="token punctuation">(</span>e <span class="token keyword">=&gt;</span> <span class="token punctuation">(</span>e<span class="token punctuation">.</span>_1<span class="token punctuation">,</span> e<span class="token punctuation">.</span>_2<span class="token punctuation">.</span>size<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>

    sparkContext<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="spark运行方式" tabindex="-1"><a class="header-anchor" href="#spark运行方式" aria-hidden="true">#</a> Spark运行方式</h2><ol><li><p>Local</p><p>单体</p></li><li><p>Standalone</p><p>集群</p></li><li><p>Yarn</p><p>Spark主计算，Yarn主资源调度</p></li><li><p>K8S</p></li></ol><h2 id="spark核心模块" tabindex="-1"><a class="header-anchor" href="#spark核心模块" aria-hidden="true">#</a> Spark核心模块</h2><ol><li>Apache Spark Core</li><li>Spark SQL</li><li>Spark Streaming</li></ol><h2 id="spark运行架构" tabindex="-1"><a class="header-anchor" href="#spark运行架构" aria-hidden="true">#</a> Spark运行架构</h2><p>Driver是master，Executor是slave(帕鲁)</p><figure><img src="`+p+'" alt="image-20240306104245319" tabindex="0" loading="lazy"><figcaption>image-20240306104245319</figcaption></figure><h3 id="driver" tabindex="-1"><a class="header-anchor" href="#driver" aria-hidden="true">#</a> Driver</h3><p>用于执行Spark任务中的main方法，负责实际代码的执行工作。</p><p>Driver在Spark作业执行时主要负责：</p><ol><li>将用户程序转化为job</li><li>在Executor之间调度task</li><li>跟踪Executor的执行情况</li><li>通过UI展示查询运行情况</li></ol><h3 id="executor" tabindex="-1"><a class="header-anchor" href="#executor" aria-hidden="true">#</a> Executor</h3><p>是集群中工作节点（Worker）中的一个JVM进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。</p><p>Executor有两个核心功能：</p><ol><li>负责运行组成Spark应用的任务，并将结果返回给驱动器进程</li><li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</li></ol><h3 id="master-worker" tabindex="-1"><a class="header-anchor" href="#master-worker" aria-hidden="true">#</a> Master &amp; Worker</h3><p>Spark集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master和Worker，这里的Master是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于Yarn环境中的RM, 而Worker呢，也是进程，一个Worker运行在集群中的一台服务器上，由Master分配资源对数据进行并行的处理和计算，类似于Yarn环境中NM。</p><p>PS：driver可能工作在Worker节点，也可能工作在Master节点</p><h3 id="applicationmaster" tabindex="-1"><a class="header-anchor" href="#applicationmaster" aria-hidden="true">#</a> ApplicationMaster</h3><p>Hadoop用户向YARN集群提交应用程序时,提交程序中应该包含ApplicationMaster，用于向资源调度器申请执行任务的资源容器Container，运行用户自己的程序任务job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。<br> 说的简单点就是，ResourceManager（资源）和Driver（计算）之间的解耦合靠的就是ApplicationMaster，Driver申请资源不会直接找Master而是找ApplicationMaster帮忙找Master。</p><h3 id="dag" tabindex="-1"><a class="header-anchor" href="#dag" aria-hidden="true">#</a> DAG</h3><p>大数据计算引擎框架我们根据使用方式的不同一般会分为四类，其中第一类就是Hadoop所承载的MapReduce,它将计算分为两个阶段，分别为 Map阶段 和 Reduce阶段。对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。 由于这样的弊端，催生了支持 DAG 框架的产生。</p><figure><img src="'+i+'" alt="image-20240306111636926" tabindex="0" loading="lazy"><figcaption>image-20240306111636926</figcaption></figure><p>支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的 Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 <mark>Oozie</mark> 来说，大多还是批处理的任务。接下来就是以 Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要是 Job 内部的 DAG 支持（不跨越 Job），以及实时计算。<br> 这里所谓的有向无环图，并不是真正意义的图形，而是由Spark程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来,这样更直观，更便于理解，可以用于表示程序的拓扑结构。</p><p>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。</p><h2 id="spark-job提交流程" tabindex="-1"><a class="header-anchor" href="#spark-job提交流程" aria-hidden="true">#</a> Spark Job提交流程</h2><figure><img src="'+t+'" alt="image-20240306164313345" tabindex="0" loading="lazy"><figcaption>image-20240306164313345</figcaption></figure><h2 id="spark核心编程" tabindex="-1"><a class="header-anchor" href="#spark核心编程" aria-hidden="true">#</a> Spark核心编程</h2><p>Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p><ul><li>RDD：弹性分布式数据集</li><li>累加器：分布式共享只写变量</li><li>广播变量：分布式共享只读变量</li></ul><h2 id="rdd概念" tabindex="-1"><a class="header-anchor" href="#rdd概念" aria-hidden="true">#</a> RDD概念</h2><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p><ul><li>弹性 <ul><li>储存的弹性：内存和磁盘的自动切换</li><li>容错的弹性：数据丢失可以自动恢复</li><li>计算的弹性：计算出错重试机制</li><li>分片的弹性：可根据需要重新分片</li></ul></li><li>分布式：数据存储在大数据集群不同节点上</li><li>数据集：RDD封装了计算逻辑，并不保存数据</li><li>数据抽象：RDD 是一个抽象类，需要子类具体实现</li><li>不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD ，在新的 RDD 里面封装计算逻辑</li><li>可分区、并行计算</li></ul><h3 id="核心属性" tabindex="-1"><a class="header-anchor" href="#核心属性" aria-hidden="true">#</a> 核心属性</h3><ul><li><p>分区列表</p><p>RDD数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。</p></li><li><p>分区计算函数</p><p>Spark在计算时，是使用分区函数对每一个分区进行计算。</p></li><li><p>RDD之间的依赖关系</p><p>RDD是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个RDD建立依赖关系。</p></li><li><p>分区器（可选）</p><p>当数据为KV类型数据时，可以通过设定分区器自定义数据的分区。</p></li><li><p>首选位置（可选）</p><p>计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算。</p></li></ul><h3 id="执行原理" tabindex="-1"><a class="header-anchor" href="#执行原理" aria-hidden="true">#</a> 执行原理</h3><p>从计算的角度来讲，数据处理过程中需要计算资源（内存&amp;CPU）和计算模型（逻辑）。</p><p>执行时，需要将计算资源和计算模型进行协调和整合。</p><p>Spark框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上，按照指定的计算模型进行数据计算，最后得到计算结果。</p><p>RDD是Spark框架中用于数据处理的核心模型，接下来我们看看，在Yarn环境中，RDD的工作原理</p><ol><li><p>启动Yarn集群环境</p><figure><img src="'+l+'" alt="image-20240306170546483" tabindex="0" loading="lazy"><figcaption>image-20240306170546483</figcaption></figure></li><li><p>Spark申请资源创建Driver和Executor</p><figure><img src="'+o+'" alt="image-20240306170605695" tabindex="0" loading="lazy"><figcaption>image-20240306170605695</figcaption></figure></li><li><p>Spark将计算逻辑根据分区划分成不同的Task</p><figure><img src="'+r+'" alt="image-20240306171631987" tabindex="0" loading="lazy"><figcaption>image-20240306171631987</figcaption></figure></li><li><p>Driver将任务根据计算节点状态发送到对应的Executor进行计算</p><figure><img src="'+c+`" alt="image-20240306171657469" tabindex="0" loading="lazy"><figcaption>image-20240306171657469</figcaption></figure></li></ol><p>从以上流程可以看出RDD在整个流程中主要用于将逻辑进行封装，并生成Task发送给Executor执行计算，接下来我们就一起看看Spark框架中RDD是具体是如何进行数据处理的。</p><h3 id="rdd的创建" tabindex="-1"><a class="header-anchor" href="#rdd的创建" aria-hidden="true">#</a> RDD的创建</h3><ul><li>从集合创建RDD</li><li>从文件创建RDD</li><li>从RDD创建RDD</li><li>直接创建RDD</li></ul><h3 id="rdd的并行度和分区" tabindex="-1"><a class="header-anchor" href="#rdd的并行度和分区" aria-hidden="true">#</a> RDD的并行度和分区</h3><p>默认情况下，Spark可以将一个作业切分多个任务后，发送给Executor节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建RDD时指定。记住，这里的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。</p><p>指定并行度：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>val dataRDD: RDD[Int] = sparkContext.makeRDD(List(1,2,3,4))
val fileRDD: RDD[String] = sparkContext.textFile(&quot;input&quot;,2)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>指定并行度后，数据会按照并行度来进行分区，但文件的分区并不是单纯的除以并行度</p><h2 id="rdd-transform-operation" tabindex="-1"><a class="header-anchor" href="#rdd-transform-operation" aria-hidden="true">#</a> RDD transform operation</h2><ul><li><p>map</p></li><li><p>mapPartitions</p></li><li><p>mapPartitionsWithIndex</p></li><li><p>flatMap</p></li><li><p>glom</p></li><li><p>groupBy</p></li><li><p>filter</p></li><li><p>sample</p></li><li><p>distinct</p></li><li><p>coalesce</p></li><li><p>repartition</p></li><li><p>sortBy</p></li><li><p>intersection</p></li><li><p>union</p></li><li><p>subtract</p></li><li><p>zip</p></li><li><p>partitionBy</p></li><li><p>reduceByKey</p></li><li><p>groupByKey</p></li><li><p>aggregateByKey</p></li><li><p>foldByKey</p></li><li><p>combineByKey</p></li><li><p>sortByKey</p></li><li><p>join</p></li><li><p>leftOuterJoin</p></li><li><p>cogroup</p></li></ul><h2 id="rdd-action-operation" tabindex="-1"><a class="header-anchor" href="#rdd-action-operation" aria-hidden="true">#</a> RDD action operation</h2><ul><li>reduce</li><li>collect</li><li>count</li><li>first</li><li>take</li><li>takeOrdered</li><li>aggregate</li><li>fold</li><li>countByKey</li><li>save <ul><li>saveAsTextFile</li><li>saveAsObjectFile</li><li>saveAsSequenceFile</li></ul></li><li>foreach</li></ul><h2 id="rdd-序列化" tabindex="-1"><a class="header-anchor" href="#rdd-序列化" aria-hidden="true">#</a> RDD 序列化</h2><h3 id="闭包-closure" tabindex="-1"><a class="header-anchor" href="#闭包-closure" aria-hidden="true">#</a> 闭包（Closure）</h3><p>闭包（Closure）是一个在编程中的概念，特别是在函数式编程语言中。它指的是一个函数捕获了除自己参数列表以外的其他变量，并能够访问和操作这些变量的能力。闭包通常是在一个函数内部创建另一个函数时形成的，内部函数将持有并记住创建时作用域中的变量，即使外部函数已经执行完毕。</p><p>这些被内部函数捕获的变量通常被称为“自由变量”，它们不是内部函数的参数，也不是局部变量，但内部函数依然可以访问它们。闭包的强大之处在于，即使外部函数的执行上下文已经消失，闭包仍然可以记住和访问这些自由变量的值。</p><p>闭包的一个经典例子是在一个函数中返回另一个函数，如下所示（使用JavaScript）：</p><div class="language-javascript line-numbers-mode" data-ext="js"><pre class="language-javascript"><code><span class="token keyword">function</span> <span class="token function">createAdder</span><span class="token punctuation">(</span><span class="token parameter">x</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token keyword">return</span> <span class="token keyword">function</span><span class="token punctuation">(</span><span class="token parameter">y</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">return</span> x <span class="token operator">+</span> y<span class="token punctuation">;</span>
  <span class="token punctuation">}</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token keyword">var</span> addFive <span class="token operator">=</span> <span class="token function">createAdder</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token function">addFive</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// 输出 7</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的例子中，<code>createAdder</code>函数返回了一个内部函数，这个内部函数使用了<code>createAdder</code>的参数<code>x</code>。即使<code>createAdder</code>的执行上下文结束后，返回的内部函数仍然能够访问变量<code>x</code>，这是因为闭包的存在。</p><p>闭包在很多编程语言中都有应用，包括JavaScript、Python、Ruby、Scala等，它们在事件处理、回调函数、函数柯里化等场景中特别有用。</p><h3 id="序列化-serializable" tabindex="-1"><a class="header-anchor" href="#序列化-serializable" aria-hidden="true">#</a> 序列化（Serializable）</h3><p>在Apache Spark中，序列化是指将对象转换为可以进行网络传输或写入磁盘的格式的过程。由于Spark是一个分布式计算系统，它需要在不同的节点之间传递数据和任务（例如函数和闭包），因此序列化在Spark中非常重要。</p><p>Spark中的序列化主要用于以下场景：</p><ol><li><p><strong>分布式任务传输</strong>:<br> 当Spark将任务（例如map、reduce函数等）发送到集群中的不同节点执行时，这些任务需要被序列化以便传输。</p></li><li><p><strong>数据持久化</strong>:<br> 当数据需要存储到磁盘（例如通过RDD的<code>persist</code>或<code>cache</code>方法）时，它们需要被序列化。</p></li><li><p><strong>Shuffle操作</strong>:<br> 在shuffle操作（如groupBy、reduceByKey等）期间，数据在不同节点间进行重新分配，需要序列化以便网络传输。</p></li></ol><p>Spark支持两种序列化库：</p><ul><li><strong>Java序列化</strong>: 使用Java的内置序列化机制。它简单易用，但通常比较慢，且序列化后的数据占用的空间较大。</li><li><strong>Kryo序列化</strong>: Kryo是一个快速且高效的序列化框架。Spark提供了对Kryo的支持，推荐在性能敏感的应用中使用。Kryo通常比Java序列化更快，且序列化后的数据更小。但是，它可能需要额外的配置，比如注册自定义类。</li></ul><p>为了使用Kryo序列化，你需要在Spark的配置中指定使用Kryo，并且可能需要注册那些要被序列化的自定义类。例如：</p><div class="language-scala line-numbers-mode" data-ext="scala"><pre class="language-scala"><code><span class="token keyword">val</span> conf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>
conf<span class="token punctuation">.</span>set<span class="token punctuation">(</span><span class="token string">&quot;spark.serializer&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span><span class="token punctuation">)</span>
conf<span class="token punctuation">.</span>registerKryoClasses<span class="token punctuation">(</span>Array<span class="token punctuation">(</span>classOf<span class="token punctuation">[</span>MyCustomClass<span class="token punctuation">]</span><span class="token punctuation">,</span> classOf<span class="token punctuation">[</span>AnotherClass<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> sc <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>conf<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在使用Spark时，合理地使用序列化可以显著提高性能，尤其是在网络传输和磁盘I/O方面。</p><h2 id="rdd依赖关系" tabindex="-1"><a class="header-anchor" href="#rdd依赖关系" aria-hidden="true">#</a> RDD依赖关系</h2><h3 id="rdd-lineage" tabindex="-1"><a class="header-anchor" href="#rdd-lineage" aria-hidden="true">#</a> RDD Lineage</h3><p>RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p><h3 id="rdd窄依赖" tabindex="-1"><a class="header-anchor" href="#rdd窄依赖" aria-hidden="true">#</a> RDD窄依赖</h3><p>窄依赖表示每一个父(上游)RDD的Partition最多被子（下游）RDD的一个Partition使用，窄依赖我们形象的比喻为独生子女。</p><h3 id="rdd宽依赖" tabindex="-1"><a class="header-anchor" href="#rdd宽依赖" aria-hidden="true">#</a> RDD宽依赖</h3><p>宽依赖表示同一个父（上游）RDD的Partition被多个子（下游）RDD的Partition依赖，会引起Shuffle，总结：宽依赖我们形象的比喻为多生。</p><h3 id="rdd任务划分" tabindex="-1"><a class="header-anchor" href="#rdd任务划分" aria-hidden="true">#</a> RDD任务划分</h3><p>RDD任务切分中间分为：Application、Job、Stage和Task</p><ul><li><p>Application：初始化一个SparkContext即生成一个Application；</p></li><li><p>Job：一个Action算子就会生成一个Job；</p></li><li><p>Stage：Stage等于宽依赖(ShuffleDependency)的个数加1；</p></li><li><p>Task：一个Stage阶段中，最后一个RDD的分区个数就是Task的个数。</p><p>注意：Application-&gt;Job-&gt;Stage-&gt;Task每一层都是1对n的关系。</p></li></ul><h2 id="rdd持久化" tabindex="-1"><a class="header-anchor" href="#rdd持久化" aria-hidden="true">#</a> RDD持久化</h2><h3 id="cache" tabindex="-1"><a class="header-anchor" href="#cache" aria-hidden="true">#</a> Cache</h3><p>RDD通过Cache或者Persist方法将前面的计算结果缓存，默认情况下会把数据以缓存在JVM的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action算子时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p><p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p><p>Spark会自动对一些Shuffle操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点Shuffle失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用persist或cache。</p><h3 id="checkpoint" tabindex="-1"><a class="header-anchor" href="#checkpoint" aria-hidden="true">#</a> CheckPoint</h3><p>所谓的检查点其实就是通过将RDD中间结果写入磁盘，由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错。如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。<br> 对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p><h3 id="cache-vs-checkpoint" tabindex="-1"><a class="header-anchor" href="#cache-vs-checkpoint" aria-hidden="true">#</a> Cache VS CheckPoint</h3><p>在Apache Spark中，<code>cache()</code>和<code>checkpoint()</code>都是用来优化和提高Spark作业的性能的技术，但它们在目的和使用方式上有所不同：</p><ol><li><p><strong>Cache</strong>:</p><ul><li><code>cache()</code>方法用于将RDD或DataFrame/Dataset的数据存储在内存中，以便后续的动作（action）可以更快地访问这些数据。如果内存不足，Spark会使用LRU（最近最少使用）策略将部分数据溢写到磁盘。</li><li>缓存是懒惰执行的，仅当对RDD执行动作时才会真正缓存数据。</li><li>缓存有助于加速迭代算法和交互式数据分析，因为它减少了对相同RDD的重复计算。</li><li>缓存不会切断RDD的血统（lineage），即缓存的RDD仍然保留了从原始数据到当前状态的所有转换信息。</li></ul></li><li><p><strong>Checkpoint</strong>:</p><ul><li><code>checkpoint()</code>方法用于将RDD或DataFrame/Dataset的数据存储到一个可靠的分布式存储系统（如HDFS）中，并切断RDD的血统。这意味着，一旦数据被检查点保存，Spark就不需要保留原始数据的所有转换历史。</li><li>检查点也是懒惰执行的，需要在调用<code>checkpoint()</code>方法后执行一个动作才会触发实际的检查点操作。</li><li>检查点主要用于避免长血统链带来的性能问题，以及在容错机制中防止血统链过长导致的计算代价过高。</li><li>检查点需要用户提前设置检查点目录，Spark将数据写入该目录。</li></ul></li></ol><p>总结来说，<code>cache()</code>主要用于加速数据访问，而<code>checkpoint()</code>用于截断血统并提供容错支持。在实际使用中，根据应用程序的需求和资源限制选择合适的方法非常重要。有时候，开发者可能会组合使用缓存和检查点以达到最佳的性能和容错平衡。</p><h2 id="rdd分区器" tabindex="-1"><a class="header-anchor" href="#rdd分区器" aria-hidden="true">#</a> RDD分区器</h2><p>Spark目前支持Hash分区和Range分区，和用户自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区，进而决定了Reduce的个数。<br> 只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None<br> 每个RDD的分区ID范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。</p><h2 id="累加器" tabindex="-1"><a class="header-anchor" href="#累加器" aria-hidden="true">#</a> 累加器</h2><p>累加器用来把Executor端变量信息聚合到Driver端。在Driver程序中定义的变量，在Executor端的每个Task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行merge。</p><h2 id="广播变量" tabindex="-1"><a class="header-anchor" href="#广播变量" aria-hidden="true">#</a> 广播变量</h2><p>在Apache Spark中，&quot;广播&quot;是指将数据分发到集群中的所有节点的过程，而&quot;广播变量&quot;是这种广播机制的一个具体实现，用于在集群的所有节点之间高效地共享一个只读变量。</p><p>广播变量用于以下场景：</p><ol><li><p><strong>大数据共享</strong>:<br> 当一个大的只读数据集（如一个大数组或一个大型的查找表）需要在多个Spark任务之间共享时，将这个数据集作为广播变量来共享是非常有用的。这样可以避免将数据集的多个副本发送到同一个节点上的不同任务。</p></li><li><p><strong>优化网络通信</strong>:<br> 如果不使用广播变量，Spark的每个任务可能会从驱动程序节点中获取需要的数据，这会导致大量的网络传输，并可能成为性能瓶颈。使用广播变量可以显著减少网络传输的数据量。</p></li><li><p><strong>减少任务序列化成本</strong>:<br> 如果在闭包中直接使用大型数据结构，那么每个任务都需要对这个数据结构进行序列化和传输。而广播变量则只需要序列化和传输一次。</p></li></ol><p>创建广播变量的代码示例（使用Scala）：</p><div class="language-scala line-numbers-mode" data-ext="scala"><pre class="language-scala"><code><span class="token keyword">val</span> sparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> largeLookupTable <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token comment">// 某个大型的数据结构</span>
<span class="token keyword">val</span> broadcastVar <span class="token operator">=</span> sparkContext<span class="token punctuation">.</span>broadcast<span class="token punctuation">(</span>largeLookupTable<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在集群中的任务中使用广播变量：</p><div class="language-scala line-numbers-mode" data-ext="scala"><pre class="language-scala"><code>rdd<span class="token punctuation">.</span>map <span class="token punctuation">{</span> x <span class="token keyword">=&gt;</span>
  <span class="token comment">// 使用 broadcastVar.value 来访问广播变量的值</span>
  <span class="token keyword">val</span> lookupResult <span class="token operator">=</span> broadcastVar<span class="token punctuation">.</span>value<span class="token punctuation">.</span>lookup<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在这个例子中，<code>broadcastVar</code>是一个广播变量，它被用来在集群中所有节点上共享<code>largeLookupTable</code>。在任务中通过<code>broadcastVar.value</code>来访问这个共享的数据。</p><p>总的来说，广播变量是Spark中一个重要的优化特性，它有助于减少网络传输，提高大规模数据处理的效率。</p>`,117),k=[u];function h(D,g){return n(),s("div",null,k)}const v=a(d,[["render",h],["__file","Spark学习笔记.html.vue"]]);export{v as default};
