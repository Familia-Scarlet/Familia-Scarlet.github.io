import{_ as a,o as p,c as l,a as i}from"./app-09c9e424.js";const e={},o=i('<h2 id="spark概述" tabindex="-1"><a class="header-anchor" href="#spark概述" aria-hidden="true">#</a> Spark概述</h2><p>Spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。</p><h2 id="spark-vs-hadoop" tabindex="-1"><a class="header-anchor" href="#spark-vs-hadoop" aria-hidden="true">#</a> Spark VS Hadoop</h2><p>从时间上来看:</p><ul><li><p>Hadoop</p><ul><li><p>2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发</p></li><li><p>2008年1月，Hadoop成为Apache顶级项目</p></li><li><p>2011年，1.0正式发布</p></li><li><p>2012年3月，稳定版发布</p></li><li><p>2013年10月，发布2.X(Yarn)版本</p></li></ul></li><li><p>Spark</p><ul><li><p>2009年，Spark诞生于伯克利大学的AMPLab实验室</p></li><li><p>2010年，伯克利大学正式开源了Spark项目</p></li><li><p>2013年6月，Spark成为了Apache基金会下的项目</p></li><li><p>2014年2月，Spark以飞快的速度成为了Apache的顶级项目</p></li><li><p>2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark</p></li></ul></li></ul><p>从功能上来看:</p><ul><li>Hadoop <ul><li>Hadoop是由java语言编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架</li><li>作为Hadoop分布式文件系统，==HDFS==处于Hadoop生态圈的最下层，存储着所有的数据，支持着Hadoop的所有服务。它的理论基础源于Google的The Google File System这篇论文，它是GFS的开源实现</li><li>==MapReduce==是一种编程模型，Hadoop根据Google的MapReduce论文将其实现， 作为Hadoop的分布式计算模型，是Hadoop的核心。基于这个框架，分布式并行程序的编写变得异常简单。综合了HDFS的分布式存储和MapReduce的分布式计算，Hadoop在处理海量数据时，性能横向扩展变得非常容易。</li><li>==HBase==是对Google的Bigtable的开源实现，但又和Bigtable存在许多不同之处。HBase是一个基于HDFS的分布式数据库，擅长实时地随机读/写超大规模数据集。它也是Hadoop非常重要的组件。</li></ul></li><li>Spark <ul><li>Spark是一种由Scala语言开发的快速、通用、可扩展的大数据分析引擎</li><li>==Spark Core==中提供了Spark最基础与最核心的功能</li><li>==Spark SQL==是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用 SQL或者Apache Hive版本的SQL 方言（HQL）来查询数据。</li><li>==Spark Streaming==是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。</li></ul></li></ul><p>从数据处理框架看：</p><ol><li>Hadoop的MapReduce框架和Spark框架都是数据处理框架</li><li>Hadoop MapReduce由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多 并行运行的数据可复用场景（如：机器学习、图挖掘算法、交互式数据挖掘算法）中存在诸多计算效率等问题。所以Spark应运而生，Spark就是在传统的MapReduce计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的RDD计算模型。</li><li>机器学习中 ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。而Spark所基于的Scala 语言恰恰擅长函数的处理。</li><li>Spark是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集（Resilient Distributed Datasets），提供了比MapReduce丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。</li><li>Spark和Hadoop的根本差异是多个作业之间的数据通信问题: Spark 多个作业之间数据 通信是基于内存，而 Hadoop 是基于磁盘。</li><li>Spark Task 的启动时间快。Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程的方式。</li><li>Spark只有在shuffle的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交 互都要依赖于磁盘交互</li><li>Spark的缓存机制比HDFS的缓存机制高效。</li></ol><p>经过上面的比较，我们可以看出在绝大多数的数据计算场景中，Spark 确实会比 MapReduce 更有优势。但是 Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致 Job 执行失败，此时，MapReduce其实是一个更好的选择，所以 Spark 并不能完全替代 MR。</p><h2 id="spark核心模块" tabindex="-1"><a class="header-anchor" href="#spark核心模块" aria-hidden="true">#</a> Spark核心模块</h2><ol><li>Apache Spark Core</li><li>Spark SQL</li><li>Spark Streaming</li></ol>',12),r=[o];function S(d,k){return p(),l("div",null,r)}const t=a(e,[["render",S],["__file","Spark学习笔记.html.vue"]]);export{t as default};
